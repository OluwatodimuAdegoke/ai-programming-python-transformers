{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CMWMOaDg-ZBv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\oluwa\\Documents\\Github\\ai-programming-python-transformers\\lesson-1-introduction-to-transformer-neural-networks\\exercises\\1-data-loader\\starter\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "# Check the current working directory\n",
    "print(f\"Current working directory: {Path.cwd()}\")\n",
    "\n",
    "# Update the file path to the correct location\n",
    "text = Path(r'C:\\Users\\oluwa\\Documents\\Github\\ai-programming-python-transformers\\lesson-1-introduction-to-transformer-neural-networks\\demos\\tiny_shakespare.txt').read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IwwOe-tJ-xcE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Complete Works of William Shakespeare\n",
      "\n",
      "Author: William Shakespeare\n",
      "\n",
      "Release date: January 1, 1994 [eBook #100]\n",
      "                Most recently updated: October 29, 2024\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK THE COMPLETE WORKS OF WILLIAM SHAKESPEARE ***\n",
      "The Complete Works of William Shakespeare\n",
      "\n",
      "by William Shakespeare\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                    Contents\n",
      "\n",
      "    THE SONNETS\n",
      "    ALL’S WELL THAT ENDS WELL\n",
      "    THE TRAGEDY OF ANTONY AND CLEOPATRA\n",
      "    AS YOU LIKE IT\n",
      "    THE COMEDY OF ERRORS\n",
      "    THE TRAGEDY OF CORIOLANUS\n",
      "    CYMBELINE\n",
      "    THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\n",
      "    THE FIRST PART OF KING HENRY THE FOURTH\n",
      "    THE SECOND PART OF KING HENRY THE FOURTH\n",
      "    THE LIFE OF KING HENRY THE FIFTH\n",
      "    THE FIRST PART OF HENRY THE SIXTH\n",
      "    THE SECOND PART OF KING HENRY THE SIXTH\n",
      "    THE THIRD PART OF KING HENRY THE SIXTH\n",
      "    KING HENRY THE EIGHTH\n",
      "    THE LIFE AND DEATH OF KING JOHN\n",
      "    THE TRAGEDY OF JULIUS CAESAR\n",
      "    THE TRAGEDY OF KING LEAR\n",
      "    LOVE’S LABOUR’S\n"
     ]
    }
   ],
   "source": [
    "print(text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ap_Ixr0M-0Yv"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CharTokenizer:\n",
    "  def __init__(self, vocabulary):\n",
    "    self.token_id_for_char = {char: token_id for token_id, char in enumerate(vocabulary)}\n",
    "    self.char_for_token_id = {token_id: char for token_id, char in enumerate(vocabulary)}\n",
    "\n",
    "  @staticmethod\n",
    "  def train_from_text(text):\n",
    "    vocabulary = set(text)\n",
    "    return CharTokenizer(sorted(list(vocabulary)))\n",
    "\n",
    "  def encode(self, text):\n",
    "    token_ids = []\n",
    "    for char in text:\n",
    "      token_ids.append(self.token_id_for_char[char])\n",
    "    return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "  def decode(self, token_ids):\n",
    "    chars = []\n",
    "    for token_id in token_ids.tolist():\n",
    "      chars.append(self.char_for_token_id[token_id])\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "  def vocabulary_size(self):\n",
    "    return len(self.token_id_for_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "T3q9Dj3l-2Ja"
   },
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer.train_from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Lb1zImZr-4mY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36, 62, 69, 69, 72,  2, 80, 72, 75, 69, 61])\n",
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"Hello world\"))\n",
    "print(tokenizer.decode(tokenizer.encode(\"Hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MlTEiIqs-7Uz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 106\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {tokenizer.vocabulary_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7Qal76ig-94U"
   },
   "outputs": [],
   "source": [
    "# Step 1 - Define the `TokenIdsDataset` Class\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TokenIdsDataset(Dataset):\n",
    "  def __init__(self, data, block_size):\n",
    "    self.data = data\n",
    "    self.block_size = block_size\n",
    "\n",
    "  def __len__(self):\n",
    "    # TODO: If every position can be a start of an item,\n",
    "    # and all items should be \"block_size\", compute the size\n",
    "    # of the dataset\n",
    "    return len(self.data) - self.block_size\n",
    "\n",
    "  def __getitem__(self, pos):\n",
    "    # TODO: Check if the input position is valid\n",
    "    assert pos < len(self.data) - self.block_size\n",
    "\n",
    "    item = self.data[pos: pos + self.block_size]\n",
    "    target = self.data[pos+1: pos + 1+ self.block_size]\n",
    "    return item, target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "oPPfQ5n1_BuV"
   },
   "outputs": [],
   "source": [
    "# Step 2 - Tokenize the Text\n",
    "\n",
    "# TODO: Encode text using the tokenizer\n",
    "# Create \"TokenIdsDataset\" with the tokenized text, and block_size=64\n",
    "dataset = TokenIdsDataset(tokenizer.encode(text), 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Complete Works of William Shakespeare\n",
      "\n",
      "Author: Willia\n",
      "itle: The Complete Works of William Shakespeare\n",
      "\n",
      "Author: William\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Retrieve the First Item from the Dataset\n",
    "\n",
    "# TODO: Get the first item from the dataset\n",
    "item, target = dataset[0]\n",
    "print(tokenizer.decode(item))\n",
    "print(tokenizer.decode(target))\n",
    "# Decode \"x\" using tokenizer.decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "pU5xPNPN_RSQ"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "# RandomSampler allows to read random items from a datasset\n",
    "sampler = RandomSampler(dataset, replacement=True)\n",
    "# Dataloader will laod two random samplers using the sampler\n",
    "dataloader = DataLoader(dataset, batch_size=2, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "cmoIkxKP_gBD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64])\n",
      "ave, Bassanio, I am half yourself,\n",
      "And I must freely have the ha\n",
      "ve, Bassanio, I am half yourself,\n",
      "And I must freely have the hal\n"
     ]
    }
   ],
   "source": [
    "# Step 4 - Use a DataLoader\n",
    "\n",
    "# TODO: Get a single batch from the \"dataloader\"\n",
    "\n",
    "data, target = next(iter(dataloader))\n",
    "print(data.shape)\n",
    "print(tokenizer.decode(data[0]))\n",
    "print(tokenizer.decode(target[0]))\n",
    "# For this call the `iter` function, and pass DataLoader instance to it. This will create an iterator\n",
    "# Then call the `next` function and pass the iterator to it to get the first training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRYTdaFs_nrH"
   },
   "outputs": [],
   "source": [
    "# TODO: Decode input item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7z-wrmxL_ucH"
   },
   "outputs": [],
   "source": [
    "# TODO: Decode target item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZyqnpXW7_w__"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
