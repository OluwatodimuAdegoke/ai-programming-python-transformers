{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "cuda_is_available = torch.cuda.is_available()\n",
        "\n",
        "if cuda_is_available:\n",
        "  print(\"All good!\")\n",
        "else:\n",
        "  print(\"CUDA is NOT available!\")"
      ],
      "metadata": {
        "id": "ulgnkbbvEt6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can use this prompt or try something else!\n",
        "prompt = \"During the latest presentation OpenAI\"\n",
        "# A good model for this exercise, but feel free to use another model: https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads\n",
        "model = \"openai-community/gpt2-large\""
      ],
      "metadata": {
        "id": "7R6RnqQu0-P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Import \"pipeline\" function\n",
        "\n",
        "# TODO: Create a pipeline for text generation, selected model, and \"device=0\" (to use CUDA)"
      ],
      "metadata": {
        "id": "hkVKzDegAItu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Generate text with the selected prompt\n",
        "# Recommended parameters:\n",
        "# max_length=100 - generate up to 100 tokens\n",
        "# num_return_sequences=1 - only return one generated sequence\n"
      ],
      "metadata": {
        "id": "_xUAq3XiAN1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now try different parameters\n",
        "\n",
        "# TODO: Try text generation with \"do_sample\" parameter equal to `True` or `False`\n"
      ],
      "metadata": {
        "id": "rGcDDPcz1Ln5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Try text generation using \"Beam-search strategy\"\n",
        "# Parameters:\n",
        "# do_sample=False\n",
        "# num_beams - try different values"
      ],
      "metadata": {
        "id": "uo3CC3yk4QmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Try text generation using \"Beam-search multinomial sampling\"\n",
        "# Parameters:\n",
        "# do_sample=True\n",
        "# num_beams - try different values"
      ],
      "metadata": {
        "id": "K96hVRPC6MUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Try text generation with different \"top_k\" values. E.g. from 1 to 500"
      ],
      "metadata": {
        "id": "f5R7GeD3AUgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Try text generation with different \"temperature\" values. E.g. from 0.1 to 3.0"
      ],
      "metadata": {
        "id": "yXOuaxe0RJwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UUiu5ZdT9ArW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}